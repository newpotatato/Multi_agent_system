# Детальный анализ алгоритмов многоагентной системы

## Оглавление
1. [Функции ошибки](#функции-ошибки)
2. [Признаки задач](#признаки-задач)
3. [Определение сложности](#определение-сложности)
4. [Статистики выполнения](#статистики-выполнения)
5. [Параметры нормализации](#параметры-нормализации)
6. [Гиперпараметры алгоритмов](#гиперпараметры-алгоритмов)
7. [Анализ сходимости](#анализ-сходимости)

---

## Функции ошибки

### Функция ошибки для предсказания времени обработки

**Точная математическая формулировка:**

```
F_proc(θ) = mean( ((p̂ + x^T θ - p_real) / (h(T) * p_real + ε))² )
```

**Где:**
- `p̂` - предсказанная нагрузка модели predict_load()
- `x^T θ` - скалярное произведение вектора признаков на параметры оптимизации
- `p_real` - фактическая нагрузка, вычисленная как: 
  ```
  p_real = max(0, min(1, base_real_load + noise))
  base_real_load = 0.3 * norm_length + 0.7 * norm_complexity
  noise = uniform(-0.05, 0.05)
  ```
- `h(T) = 0.1` - коэффициент нормализации времени
- `ε = 1e-6` - малый параметр для численной устойчивости

**Алгоритм вычисления:**

```python
def _loss_function_proc(self, theta):
    if len(self.history) < 5:
        return 0.5  # Базовое значение
    
    losses = []
    h_T = 0.1  # Коэффициент нормализации
    epsilon = 1e-6
    
    for prompt, p_hat, D, p_real in self.history[-10:]:
        x_i_theta = sum(x * t for x, t in zip(prompt['features'], theta))
        predicted = p_hat + x_i_theta
        normalizer = h_T * p_real + epsilon
        error = ((predicted - p_real) / normalizer) ** 2
        losses.append(error)
    
    return np.mean(losses) if losses else 0.5
```

### Функция ошибки для времени ожидания

**Точная математическая формулировка:**

```
F_wait(θ) = mean( ((ŵ + x^T θ - w_real)² * (1 - s/s_max)) )
```

**Где:**
- `ŵ` - предсказанное время ожидания от predict_waiting_time()
- `w_real` - фактическое время ожидания (по умолчанию равно ŵ)
- `s` - успешность выполнения (по умолчанию 0.9)
- `s_max = 1.0` - максимальная успешность

**Пороговые условия:**
- Минимальная история для вычисления: 5 записей
- Используются последние 10 записей истории
- Базовое значение при недостатке данных: 0.5

---

## Признаки задач

### Структура признаков

**Размерность признаков:**
- По умолчанию: 5 или 10 признаков на задачу
- Адаптивная размерность в зависимости от конфигурации MODEL_PARAMS

**Генерация признаков:**
```python
# Если признаки не заданы явно
features = np.random.random(5)  # или 10

# Веса для 5 признаков
weights = [0.3, 0.2, 0.25, 0.15, 0.1]

# Веса для 10 признаков  
weights = [0.15, 0.12, 0.13, 0.1, 0.08, 0.12, 0.1, 0.08, 0.07, 0.05]
```

**Диапазоны значений:**
- Все признаки нормализованы в диапазоне [0, 1]
- Генерируются случайно при отсутствии явного определения
- Используются для вычисления скалярного произведения x^T θ

### Классификация задач

**9 типов задач с весовыми коэффициентами:**

| Тип | Описание | Вес | Примеры ключевых слов |
|-----|----------|-----|----------------------|
| math | Математические вычисления | 1.2 | математика, формула, уравнение |
| code | Программирование | 1.1 | код, python, алгоритм |
| text | Текстовая обработка | 1.0 | текст, статья, перевод |
| analysis | Анализ данных | 1.0 | анализ, данные, статистика |
| creative | Творческие задачи | 0.9 | творчество, креативность, идеи |
| explanation | Объяснения понятий | 0.8 | объяснение, что такое, как работает |
| planning | Планирование | 1.0 | планирование, план, стратегия |
| research | Исследования | 0.9 | исследование, поиск, изучить |
| optimization | Оптимизация | 1.1 | оптимизация, улучшение, эффективность |

**Алгоритм классификации:**
1. **Анализ ключевых слов** с взвешенным подсчетом
2. **TF-IDF векторизация** с параметрами:
   - ngram_range=(1, 2)
   - max_features=1500
   - min_df=1, max_df=0.95
3. **Косинусное сходство** с описаниями типов
4. **Динамические веса**: приоритет ключевым словам при их наличии

---

## Определение сложности

### Модель определения сложности

**Входные параметры:**
- `complexity` - явно заданная сложность (1-10)
- Если не задана, используется значение по умолчанию: 5

**Связь сложности с временем выполнения:**

```python
# В MockLLMExecutor
base_time = task.get('complexity', 5) * 0.5
execution_time = base_time + random.uniform(-base_time*0.3, base_time*0.5)

# Формула для токенов
tokens = int(50 + complexity * 30 + random.randint(0, 100))
```

**Влияние на релевантность:**
```python
complexity_factor = min(complexity / 10.0, 1.0)
relevance = base_relevance * (0.7 + 0.3 * complexity_factor)
```

**Связь с p_real:**
```python
norm_complexity = min(complexity / 10, 1.0)
base_real_load = 0.3 * norm_length + 0.7 * norm_complexity
```

### Риск таймаута

**Градуированная оценка риска:**
```python
def _calculate_timeout_risk(self, execution_time):
    if execution_time > self.timeout_threshold:           # > 30с
        return 1.0
    elif execution_time > self.timeout_threshold * 0.8:   # > 24с
        return 0.7  
    elif execution_time > self.timeout_threshold * 0.5:   # > 15с
        return 0.3
    else:                                                # ≤ 15с
        return 0.1
```

---

## Статистики выполнения

### Фактическое время выполнения

**Из тестовых данных (test_results.json):**

**Диапазон времени выполнения:** 2.38 - 4.27 секунд

**Статистическое распределение:**
- Среднее время: ~3.3 секунды
- Медиана: ~3.4 секунды  
- Стандартное отклонение: ~0.7 секунды

**Детальные примеры:**
```
Task 0: 2.81с, 190 токенов, $0.190
Task 1: 3.98с, 271 токенов, $0.271
Task 2: 2.38с, 222 токена, $0.222
Task 3: 4.27с, 274 токена, $0.274
```

### Статистики с реальными LLM

**Из real_llm_execution_summary.json:**

**Производительность по типам задач:**
- math: 2 задачи, 100% успех
- text: 1 задача, 100% успех
- analysis: 1 задача, 100% успех
- creative: 1 задача, 100% успех
- explanation: 1 задача, 100% успех
- planning: 1 задача, 100% успех
- optimization: 1 задача, 100% успех

**Метрики исполнителей:**
```
executor_0: 3 задачи, 4.07с общее время, $0.547
executor_1: 3 задачи, 2.92с общее время, $0.780  
executor_2: 2 задачи, 1.68с общее время, $0.256
```

**Общие метрики:**
- Средняя длительность: 1.08 секунды
- Общая стоимость: $1.583
- Средняя стоимость за задачу: $0.198

### Расширенная статистика

**Из enhanced_broker_comparison_results.json (500 задач):**

**Распределение по типам задач:**
- math, code, text, analysis, creative, explanation
- planning, research, optimization, debugging, testing
- documentation, translation, summarization, classification

**Метрики производительности:**
- Время обработки: 0.0002-0.001 секунды на задачу
- Успешность: ~95% (24 неуспешных из 500)
- Приоритеты: 2-10 уровней
- Сложность: 3-10 уровней

---

## Параметры нормализации

### Коэффициенты устойчивости

**В функции ошибки обработки:**
- `h(T) = 0.1` - коэффициент нормализации времени
- `ε = 1e-6` - параметр численной устойчивости
- Базовое значение при недостатке истории: `0.5`

**В функции ошибки ожидания:**
- `s_max = 1.0` - максимальная успешность
- Успешность по умолчанию: `s = 0.9`
- Весовой коэффициент: `weight = 1 - (s / s_max)`

### Пороговые значения

**Минимальные требования:**
- Минимальная история для loss функций: 5 записей
- Минимальная история для SPSA обновления: 10 записей
- Окно истории для вычислений: последние 10 записей

**Нормализация нагрузки:**
```python
# Нормализация длины текста
norm_length = min(text_length / 1000, 1.0)

# Нормализация сложности  
norm_complexity = min(complexity / 10, 1.0)

# Финальная нормализация
p_real = max(0.0, min(1.0, base_real_load + noise))
```

**Временные пороги:**
- Timeout threshold: 30.0 секунд
- Максимальное ожидание: без ограничений
- Минимальное время ожидания: 0.1 секунды

---

## Гиперпараметры алгоритмов

### SPSA параметры

```python
SPSA_PARAMS = {
    'alpha': 0.01,              # Скорость обучения
    'beta': 0.1,                # Размер возмущения  
    'gamma_consensus': 0.02,    # Коэффициент консенсуса
    'update_frequency': 10,     # Частота обновления (задач)
}
```

**Алгоритм обновления параметров:**

```python
# 1. Генерация возмущения
delta = 2 * np.random.randint(2, size=len(theta)) - 1  # {-1, +1}^d

# 2. Возмущенные параметры
theta_plus = theta + beta * delta
theta_minus = theta - beta * delta

# 3. Оценка функций потерь
f_plus = combined_loss_function(theta_plus)
f_minus = combined_loss_function(theta_minus)

# 4. Приближенный градиент
grad_approx = ((f_plus - f_minus) / (2.0 * beta)) * delta

# 5. Обновление параметров
new_theta = theta - alpha * grad_approx
```

### LVP параметры

```python
LVP_PARAMS = {
    'h': 0.1,                   # Коэффициент взаимодействия соседей
    'gamma': 0.05,              # Коэффициент локального баланса
}
```

**Формула балансировки нагрузки:**

```
u_i = h * Σ(r_ij * (y_i - y_j)) - γ * (y_i - mean(y_neighbors))
```

**Где:**
- `y_i` - текущая нагрузка брокера i
- `r_ij` - вес ребра между брокерами i и j
- `y_j` - нагрузка соседнего брокера j

### Другие критические параметры

**Модель параметры:**
```python
MODEL_PARAMS = {
    'feature_dim': 10,          # Размерность признаков
    'theta_dim': 5,             # Размерность параметров θ
    'l1_lambda': 0.01,          # L1 регуляризация
    'history_size': 1000,       # Размер истории
}
```

**Исполнитель параметры:**
```python
EXECUTOR_PARAMS = {
    'max_concurrent_tasks': 3,   # Максимум одновременных задач
    'timeout_threshold': 30.0,   # Порог таймаута (сек)
    'cost_per_token': 0.001,     # Стоимость за токен
}
```

**График параметры:**
```python
GRAPH_PARAMS = {
    'edge_probability': 0.3,     # Вероятность создания ребра
    'min_neighbors': 2,          # Минимум соседей
    'max_neighbors': 5,          # Максимум соседей
    'weight_decay': 0.95,        # Затухание весов
}
```

---

## Анализ сходимости

### Статистики SPSA оптимизации

**Из test_results.json - метрики градиентов:**

**Брокер 0:**
- Изменение θ: 6.79
- Норма градиента: 679.38
- f_plus: 530.99, f_minus: 470.23
- Финальная функция потерь: 1092.11

**Брокер 1:**
- Изменение θ: 15.64
- Норма градиента: 1563.99
- f_plus: 1318.71, f_minus: 1178.83
- Финальная функция потерь: 6337.99

**Брокер 2:**
- Изменение θ: 12.55
- Норма градиента: 1255.41
- f_plus: 702.48, f_minus: 814.77
- Финальная функция потерь: 6664.62

**Брокер 3:**
- Изменение θ: 16.73
- Норма градиента: 1672.81
- f_plus: 2627.75, f_minus: 2478.12
- Финальная функция потерь: 3633.49

### Консенсус анализ

**Консенсусное обновление - изменения параметров:**
- Брокер 0: изменение на 0.233
- Брокер 1: изменение на 0.314  
- Брокер 2: изменение на 0.446
- Брокер 3: изменение на 0.362

**Средняя дистанция:** 19.55
**Сходимость:** False (не достигнута)

### Оценка констант Липшица

**На основе градиентных норм:**

**Константы сильной выпуклости (μ):**
- Оценочный диапазон: 1e-6 до 1e-4
- Основано на соотношении ||∇f||² / (f_plus - f_minus)

**Константы Липшица (L):**
- Верхняя граница: ~1700 (max градиентная норма)
- Нижняя граница: ~470 (min градиентная норма)
- Отношение L/μ: 10⁶ - 10⁹ (указывает на плохую обусловленность)

### Распределение ошибок

**Характеристики функций потерь:**
- Диапазон: 1092 - 6665
- Большая вариабельность между брокерами
- Отсутствие монотонного убывания
- Высокие значения указывают на сложность оптимизационного ландшафта